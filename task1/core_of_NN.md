# 对神经网络的基本理解:thinking:
---
## 一、多层感知机MLP的概念及结构
### 定义
前馈人工神经网络，一种最基础的神经网络结构
### **核心特点**
* **多层**：至少包含==三层==结构——一个**输入层**、一个或多个**隐藏层**、一个**输出层**。这也是与单层感知机最根本的区别
* **感知机**：源于其基本单元（==神经元==）模仿了生物神经元的行为，即对输入信号进行**加权W**求和，然后通过一个**激活函数**（引入*非线性*）产生输出
* **前馈**：指数据在网络中==单向流动==，中间没有循环或反馈连接。
### 结构与功能
通常由三部分组成：输入层、隐藏层和输出层

| 网络层 | 主要功能 | 神经元数量 | 关键操作 |
| :--- | :--- | :--- | :--- |
| **输入层** | 接收原始输入数据，并将其传递至下一层 | 由输入数据的特征数量决定<br>• 图像：28x28 像素 → 784 个神经元<br>• 表格数据：10 个特征 → 10 个神经元 | 仅负责数据分发 |
| **隐藏层** | 核心计算层，负责从数据中学习和提取复杂特征 | 层数和每层的神经元数量需根据任务调整 | 1. **加权求和**：`z = Σ(w_i * x_i) + b` <br> 2. **激活函数**：`a = f(z)` |
| **输出层** | 产生网络的最终预测结果 | 由任务类型决定：<br>• **二分类**：1 个神经元 <br>• **多分类**：与类别数相同 <br>• **回归**：1 个或多个神经元 | 将隐藏层的输出转换为可直接使用的预测结果（如概率或具体数值） |
---
## 二、数据在NN的作用
* **提供学习素材**:训练模型，发现规律
* **确定模型能力上限**：很大程度上数据的==质量==和==数量==决定一个模型最终能达到多高的性能
* **用于评估和优化**：通过将数据划分为不同部分，我们既可以训练模型，也可以评估其泛化能力，并据此调整模型参数
### 数据集
* **训练集**：直接训练模型，调整权重和偏置（通常占最大比例）
* **验证集（开发集）**： 用于评估模型表现评估模型表现，从而调整学习率、网络层数等参数
* **测试集**：用于评估模型最终泛化能力。在开发和调参过程中，必须被==封存==
### 噪声、特征与标签
* **特征**
  * 定义：描述一个数据样本的输入变量或属性。它是模型进行预测的依据
  * 例子：在预测房价的模型中，房子的“面积”、“位置”、“卧室数量”就是特征

* **标签**
  * 定义：我们想要预测的==目标输出==。在监督学习中，每个训练样本都带有标签
  * 例子：在房价预测中，房子的“真实售价”就是标签。在猫狗图片分类中，“猫”或“狗”就是标签

* **噪声**
  * 定义：数据中存在的随机、无意义的误差或干扰
  * 来源：数据收集时的误差、人为录入错误、无关背景信息等
  * 影响：可能降低其泛化性能
  * 例子：一张猫的图片中有模糊的背景杂物；房价数据中某个面积录入错误

### Batch Size & 加速原理
* 定义
一次迭代中用于计算损失并更新权重的**样本数量**
* 三种梯度下降方法

| 方法 | Batch Size | 特点 |
|------|------------|------|
| 随机梯度下降 | 1 | 更新频繁但波动大 |
| 批量梯度下降 | 整个训练集 | 计算开销大，内存要求高 |
| 小批量梯度下降 | 1 < Size < 全集 | 平衡稳定性和效率，最常用 |

* Batch 加速原理
  1. 硬件并行计算
   - GPU拥有数千核心，擅长并行计算
   - 矩阵运算可向量化处理，批量计算效率远高于串行
  2. 梯度估计更稳定
   - 批量梯度方向更准确，收敛更快
   - 可能减少训练轮次，间接加速
---
## 三、神经元
一个信息处理单元，接受输入，计算后产生输出。而神经网络就是成千上万个这样的神经元连接成层，多层堆叠在一起的
在机器学习中，简单来说，神经元就是一个 “**加权求和 + 激活函数**” 的计算单元

---
## 四、激活函数
激活函数是作用于神经元输出的函数，用于输出一个较强的信号，通过引入非线性，使得神经网络可以逼近任意复杂函数
#### 常见激活函数
##### 1. Sigmoid
**公式：**
$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**输出范围：** (0, 1)

#####  2. Tanh
**公式：**
$$\text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = \frac{e^{2x} - 1}{e^{2x} + 1}$$

**输出范围：** (-1, 1)

##### 3. ReLU (Rectified Linear Unit)
**公式：**
$$\text{ReLU}(x) = \max(0, x)$$
#### 非线性表达能力
模型学习和表示**非线性关系**的能力
使神经网络画出复杂的、弯曲的“决策边界”，从而具备强大的拟合和分类能力

---
## 五、计算图
* **定义**：计算图是一种用有向图来描述数学计算过程的工具。图中的节点代表变量（输入、参数、中间结果），边代表运算操作（加法、乘法、函数等）
* 和数据结构/离散数学中的图的**区别**：计算图是为了清晰地表达数据的流动和计算关系
* **构建**：可用圆表示变量，用边连接，边上来写计算方式
---
## 六、MLP
* **参数量**：参数量就是所有==权重==`W`和==偏置==`b`的数量之和
(输入维度 + 1) * 输出维度
+1 是偏置项
* **超参数**是在模型开始训练之前，由人工设定的配置参数
    * 隐藏层的数量、每层神经元的数量
    * 学习率、批大小、训练轮数
    * 优化器的选择、权重衰减系数、Dropout比率
---
## 七、隐藏层
**def**：输入层和输出层之间的所有神经网络层都称为隐藏层
**名字由来**：对于训练数据的提供者来说，通常只能看到网络的输入和最终的输出

---
## 八、损失函数
* **def**：衡量模型预测值与真实值之间**差距**的函数。训练模型的终极目标就是最小化这个损失函数
* 什么任务用什么损失函数，常见损失函数公式
    * **回归任务**（预测连续值）

      均方误差
    $$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

      $y_i$: 真实值
      $\hat{y}_i$: 预测值  
      $n$: 样本数量

    * **分类任务**（预测类别）

      交叉熵损失
    $$CE = -\frac{1}{n}\sum_{i=1}^{n}\sum_{c=1}^{C}y_{i,c}\log(\hat{y}_{i,c})$$
---
## 九、相关概念
* **前行传播**：从输入层，经隐藏层到输出层的**推理过程**，计算出预测值和损失
* **反向传播**：利用链式法则，从输出层开始，反向逐层计算损失函数对所有参数梯度（求偏导）
* **梯度**：梯度是一个向量，其方向指向函数值增长最快的方向
* **学习率**：梯度方向上更新参数的步长|
* **优化器**：利用梯度来更新参数的算法
    * **SGD**：最基本的梯度下降（随机梯度下降），直接沿着负梯度方向更新参数（参数减去学习率乘上损失函数梯度）
    * **SGD with Momentum**：引入“动量”，模拟物理惯性，加速收敛并减少震荡
    * **Adam**：目前最流行和默认的选择，它结合了动量和自适应学习率的优点
---
## 十、归一化和正则化
* **归一化**：将数据的分布调整到固定的尺度（常见均值为0，方差为1），如Batch Normalization
* **正则化**：给模型增加一些约束或惩罚，防止模型过拟合，提高泛化能力，如L2正则化
---
## 十一、欠拟合和过拟合
* **欠拟合**：模型太简单，无法捕捉数据中的基本规律，在训练集和测试集上的表现都很差
* **过拟合**：模型太复杂，把训练数据中的噪声和细节都学来了，在训练集上表现很好，但在未见过的测试集上表现很差