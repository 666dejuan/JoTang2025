# 机器学习核心概念简答

## 1. :gear:偏导数、链式法则、梯度、矩阵在机器学习中的作用


*   **偏导数**:triangular_ruler:（形如\( \frac{\partial f}{\partial x} \)）： 用于衡量模型损失函数随单个参数（如某个权重）变化而变化的速率（计算梯度的基础）
*   **链式法则**:link:：用于计算复合函数的导数。在**反向传播**算法中，通过链式法则将损失从输出层一层层传递回之前的层，以计算每个参数的梯度
*   **梯度**:arrow_down_small:：是所有参数偏导数的向量（==方向==是函数值变化最快的方向，==长度==代表函数值变化速率）。模型通过向梯度的**反方向**更新参数（梯度下降法）来最小化损失
*   **矩阵**:chart_with_upwards_trend:：是数据和模型参数的表示形式。将数据（如图像、文本）表示为矩阵（二阶张量）或N阶张量（简单理解为多维数组），可以通过高效的**矩阵乘法**一次性完成整个层的计算

## 2. 什么是"模型"？机器学习中的模型是如何工作的？

*   **模型** :robot:：学习从**输入数据**到**输出结果**的映射关系 \(f\)
*   **工作原理** :gear:：
    ```
    输入数据 → [模型 f(参数)] → 预测输出
                ↓
        训练调整参数 ← 损失函数反馈
    ```
    *   内部包含可调**参数**（如权重）
    *   通过**训练**不断调整参数，最小化预测误差
    *   训练完成后固定参数，用于新数据预测

## 3. :jigsaw: 模型没有意识，如何"学习"？

机器的"学习"是**数学优化**过程 :bar_chart:：

1.  **定义目标** :dart:：设定**损失函数**量化错误程度
2.  **迭代优化** :repeat:：从随机参数开始预测
3.  **计算误差** :warning:：比较预测值与真实值
4.  **反向传播** :rewind:：计算梯度，确定调整方向
5.  **参数更新** :up:：沿梯度反方向微调参数

:arrows_counterclockwise: 重复数百万次，模型参数逐渐优化，学会数据规律

## 4. 什么是监督学习？什么是无监督学习？请举例。

*   **监督学习**：使用**带明确标签**的数据进行训练。模型学习从输入到特定输出的映射
### 示例
| 场景 | 输入数据 | 标签 | 学习目标 |
|------|----------|------|----------|
| 垃圾邮件识别:email: | 邮件内容 | "垃圾"/"正常" | 判断新邮件类别 |
| 房价预测 :house:| 面积、地段、房间数 | 历史成交价 | 预测新房价格 |
| 疾病诊断 :hospital:| 症状、检查指标 | 确诊疾病 | 辅助医疗诊断 |
| 手写数字识别:writing_hand: | 图片像素 | 数字0-9 | 识别手写数字 |

*   **无监督学习**：使用**无标签**的数据进行训练，致力于发现数据中内在的==规律==
   ### 示例
| 场景 | 输入数据 | 学习目标 |
|------|----------|----------|
| 客户分群 :busts_in_silhouette:| 购买行为、用户画像 | 自动将用户分组 |
| 新闻话题聚类 :newspaper:| 文章内容 | 发现热点话题 |
| 商品推荐 :shopping_cart:| 用户浏览历史 | 发现关联商品 |

## 5. AI是什么？深度学习和传统机器学习的区别？

*   **AI（人工智能）**：是一个广泛的领域，旨在让机器能够执行通常需要人类智能的任务，如推理、学习、规划和感知。

*   **深度学习 vs  传统机器学习**：

| 特性 | 传统机器学习:older_man: | 深度学习 :baby:|
| :--- | :--- | :--- |
| **特征工程** | 依赖**人工**设计和选择数据的特征:man_office_worker:| 能够从原始数据中**自动**学习关键的特征 :mag:|
| **数据需求** | 在中小型数据集上表现良好:ok_hand: | 通常在**海量数据**:ocean:上才能发挥卓越性能|
| **计算资源** | 需求相对较低 :computer:| 需求很高，依赖GPU等硬件:rocket: |

## 6. 怎么用矩阵乘法表示神经网络的全连接层前向传播过程？

对于一个全连接层，其前向传播可以简洁地表示为：

**`Z = XW + B`**

其中：

*   **`X`**：输入矩阵，形状为 `[批量大小, 输入特征数]`，每一行是一个样本数据。
*   **`W`**：权重矩阵，形状为 `[输入特征数, 输出单元数]`。
*   **`B`**：偏置向量，形状为 `[1, 输出单元数]`（在计算中会通过**广播机制**加到每个样本上）。
*   **`Z`**：当前层的加权和输出，形状为 `[批量大小, 输出单元数]`

然后，`Z` 会经过一个**激活函数**：`A = activation(Z)`，得到该层的最终输出 `A`，并作为下一层的输入